## Алгоритмы

- Linear regression
- Euclidian K nearest neighbours
- Decision tree
- Random forest

## Данные

Все числовые признаки были нормированы, а категориальные представлены в one-hot формате. Для линейных моделей условие нормировки является обязательным, в то время как для "деревянных" алгоритмов таких ограничений нет.

## Результаты

Поскольку я мерил качество с помощью classification_report, можно посмотреть поклассовую детальную информацию по самым разным метрикам. Особенных различий обнаружено не было, кроме как в случайном лесе. Достаточно странно, поскольку я строил дерево по уму, используя беггинг на наблюдениях и crf на признаковом пространстве.

Стоит заметить, что качество линейной регрессии и алгоритма ближайших соседей полностью совпадает с реализацией из sklearn, так что можно сделать вывод, что они реализованы правильно.

Как и ожидалось, качество случайного леса превзошло стандартную регрессию и KNN.

## Вывод

В результате работы над реализацией алгоритмов машинного обучения стало понятно, что делать это стоит исключительно в образовательных целях. Не стоит "изобретать велосипед", когда существуют общедоступные реализации алгоритмов, значительно превосходящие самодельные.

В целом, качеством классификации я остался доволен, и считаю, что для такого рода задач данные алгоритмы, хоть и в разной степени, но подходят все.
